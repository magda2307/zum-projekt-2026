{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d5d5b16",
   "metadata": {},
   "source": [
    "# Notebook 2: Preprocessing Tekstu i Inżynieria Cech\n",
    "## Analiza Sentymentu Recenzji Amazon - Projekt ZUM\n",
    "\n",
    " Oczyszczenie danych zgodnie  i przygotowanie cech do modelowania.\n",
    "\n",
    "1. **Czyszczenie Tekstu:** Usuwanie HTML → Zamiana na małe litery → Usuwanie URL/Szumu → Tokenizacja → Usuwanie stopwords → **Lematyzacja**\n",
    "2. **Podział Stratyfikowany:** 70% Trening / 15% Walidacja / 15% Test \n",
    "3. **Wektoryzacja TF-IDF:** \n",
    "4. **Zapis przetworzonych danych** \n",
    "\n",
    "\n",
    "- **Lematyzacja** (spaCy): Zachowuje znaczenie semantyczne (\"running\" → \"run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db652c63",
   "metadata": {},
   "source": [
    "## 1. Import Wymaganych Bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13be8e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1/3: Downloading NLTK data...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\paula\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\paula\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK data ready\n",
      "\n",
      "============================================================\n",
      "STEP 2/3: Loading spaCy model (this takes 5-10 seconds)...\n",
      "============================================================\n",
      "spaCy model loaded in 0.64 seconds\n",
      "\n",
      "============================================================\n",
      "STEP 3/3: Setting up environment...\n",
      "============================================================\n",
      "All libraries loaded successfully\n",
      "Random seed set to 42\n",
      "spaCy optimized (disabled parser, ner for speed)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 1/3: Downloading NLTK data...\")\n",
    "print(\"=\"*60)\n",
    "nltk.download('stopwords', quiet=False)\n",
    "nltk.download('punkt', quiet=False)\n",
    "print(\"NLTK data ready\\n\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 2/3: Loading spaCy model (this takes 5-10 seconds)...\")\n",
    "print(\"=\"*60)\n",
    "start_time = time.time()\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"spaCy model loaded in {load_time:.2f} seconds\\n\")\n",
    "except OSError:\n",
    "    print(\"\\nModel not found. Downloading spaCy English model...\")\n",
    "    print(\"This will take 30-60 seconds. Please wait...\\n\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    \n",
    "    result = subprocess.run(\n",
    "        [sys.executable, '-m', 'spacy', 'download', 'en_core_web_sm'],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"Download complete! Loading model...\")\n",
    "        nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "        load_time = time.time() - start_time\n",
    "        print(f\"spaCy model downloaded and loaded in {load_time:.2f} seconds\\n\")\n",
    "    else:\n",
    "        print(\"Download failed. Please run this command manually in a terminal:\")\n",
    "        print(\"   python -m spacy download en_core_web_sm\")\n",
    "        print(\"\\nThen restart this notebook kernel and try again.\")\n",
    "        print(f\"\\nError details:\\n{result.stderr}\")\n",
    "        raise RuntimeError(\"Failed to download spaCy model\")\n",
    "\n",
    "# Set reproducibility\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 3/3: Setting up environment...\")\n",
    "print(\"=\"*60)\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"All libraries loaded successfully\")\n",
    "print(f\"Random seed set to {RANDOM_SEED}\")\n",
    "print(f\"spaCy optimized (disabled parser, ner for speed)\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869e013f",
   "metadata": {},
   "source": [
    "## 2. Wczytanie Zbalansowanego Zbioru Danych\n",
    "\n",
    "Wczytanie zbalansowanego podzbioru recenzji Amazon utworzonego w Notebooku 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0cafc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: data/processed/amazon_subset.csv\n",
      "\n",
      "============================================================\n",
      "Dataset Loaded:\n",
      "============================================================\n",
      "Total samples: 50,000\n",
      "Columns: ['text', 'label']\n",
      "Label distribution:\n",
      "label\n",
      "1    25000\n",
      "0    25000\n",
      "Name: count, dtype: int64\n",
      "============================================================\n",
      "\n",
      "Sample raw text (before preprocessing):\n",
      "I LOVE this game! My husband and I hadn't played Final Fantasy games since the old Nintendo versions, but we bought this when we bought a PlayStation 2 for our son for Xmas. Wow! Have these games chan...\n",
      "\n",
      "================================================================================\n",
      "KEY EDA FINDINGS (from Notebook 1)\n",
      "================================================================================\n",
      "\n",
      "Text Length Statistics:\n",
      "  Mean word count:     77.6 words\n",
      "  Std deviation:       42.4 words\n",
      "  95th percentile:     160 words (covers 95% of reviews)\n",
      "  99th percentile:     180 words (covers 99% of reviews)\n",
      "\n",
      "INSIGHT for Neural Networks (Notebook 3):\n",
      "   - Recommended max_length for padding: 160 words\n",
      "   - This ensures 95% of reviews fit without truncation\n",
      "   - Longer reviews will be truncated, shorter ones padded\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "data_path = 'data/processed/amazon_subset.csv'\n",
    "\n",
    "print(f\"Loading dataset from: {data_path}\")\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Dataset Loaded:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total samples: {len(df):,}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"Label distribution:\\n{df['label'].value_counts()}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(\"\\nSample raw text (before preprocessing):\")\n",
    "print(df['text'].iloc[0][:200] + \"...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY EDA FINDINGS (from Notebook 1)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df['word_count'] = df['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "percentile_95 = df['word_count'].quantile(0.95)\n",
    "percentile_99 = df['word_count'].quantile(0.99)\n",
    "mean_length = df['word_count'].mean()\n",
    "std_length = df['word_count'].std()\n",
    "\n",
    "print(f\"\\nText Length Statistics:\")\n",
    "print(f\"  Mean word count:     {mean_length:.1f} words\")\n",
    "print(f\"  Std deviation:       {std_length:.1f} words\")\n",
    "print(f\"  95th percentile:     {percentile_95:.0f} words (covers 95% of reviews)\")\n",
    "print(f\"  99th percentile:     {percentile_99:.0f} words (covers 99% of reviews)\")\n",
    "\n",
    "print(f\"\\nINSIGHT for Neural Networks (Notebook 3):\")\n",
    "print(f\"   - Recommended max_length for padding: {int(percentile_95)} words\")\n",
    "print(f\"   - This ensures 95% of reviews fit without truncation\")\n",
    "print(f\"   - Longer reviews will be truncated, shorter ones padded\")\n",
    "\n",
    "EDA_STATS = {\n",
    "    'mean_word_count': mean_length,\n",
    "    'std_word_count': std_length,\n",
    "    'percentile_95': int(percentile_95),\n",
    "    'percentile_99': int(percentile_99),\n",
    "    'recommended_max_length': int(percentile_95)\n",
    "}\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd9ca3e",
   "metadata": {},
   "source": [
    "## 3. Pipeline Czyszczenia Tekstu (Główna Funkcja)\n",
    "\n",
    "\n",
    "### **Dlaczego Lematyzacja zamiast Stemmingu?**\n",
    "| Aspekt | Lematyzacja (Używana) | Stemming (Unikany) |\n",
    "|--------|------------------------|----------------------|\n",
    "| Wynik | Prawidłowe słowa (\"running\" → \"run\") | Rdzenie, mogą nie być słowami (\"studies\" → \"studi\") |\n",
    "| Zachowanie semantyki | Wysokie (zachowuje znaczenie) | Niskie (traci znaczenie) |\n",
    "| Dokładność | Używa słownika + tagów POS | Używa heurystycznych reguł |\n",
    "| Prędkość | Wolniejsza (wymaga analizy językowej) | Szybsza (proste usuwanie sufiksów) |\n",
    "| Rekomendacja ZUM | **Preferowana dla zadań NLP** | Akceptowalna dla prostych zadań IR |\n",
    "\n",
    "**Wniosek:** Lematyzacja produkuje czystsze, bardziej znaczące cechy dla analizy sentymentu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c898f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "IDENTIFYING DOMAIN-SPECIFIC STOPWORDS (Data-Driven)\n",
      "================================================================================\n",
      "\n",
      "Top 100 words in NEGATIVE reviews: 100\n",
      "Top 100 words in POSITIVE reviews: 100\n",
      "Overlapping words (domain-specific): 73\n",
      "\n",
      "Overlapping words (appear in BOTH sentiment classes):\n",
      "   ['album', 'amazon', 'author', 'best', 'better', 'big', 'book', 'books', 'bought', 'buy', 'characters', 'day', 'did', 'didn', 'different', 'does', 'doesn', 'don', 'dvd', 'far', 'game', 'going', 'good', 'got', 'great', 'hard', 'just', 'know', 'like', 'little', 'long', 'look', 'looking', 'lot', 'love', 'make', 'movie', 'music', 'need', 'new', 'old', 'people', 'play', 'price', 'product', 'quality', 'read', 'reading', 'real', 'really', 'recommend', 'right', 'say', 'set', 'songs', 'sound', 'story', 'sure', 'thing', 'think', 'thought', 'time', 'times', 'use', 'used', 'using', 'want', 'way', 'work', 'works', 'worth', 'year', 'years']\n",
      "\n",
      "FINAL STOPWORD CONFIGURATION:\n",
      "   Standard English stopwords:  198\n",
      "   Domain-specific stopwords:   78\n",
      "   Total combined stopwords:    270\n",
      "\n",
      "Domain-specific stopwords being removed:\n",
      "   ['album', 'amazon', 'author', 'better', 'big', 'book', 'books', 'bought', 'buy', 'characters', 'day', 'delivery', 'did', 'didn', 'different', 'does', 'doesn', 'don', 'dvd', 'far', 'game', 'going', 'got', 'hard', 'item', 'items', 'just', 'know', 'like', 'little', 'long', 'look', 'looking', 'lot', 'make', 'movie', 'music', 'need', 'new', 'old', 'order', 'ordered', 'people', 'play', 'price', 'product', 'products', 'purchase', 'purchased', 'quality', 'read', 'reading', 'real', 'really', 'recommend', 'right', 'say', 'set', 'shipping', 'songs', 'sound', 'story', 'sure', 'thing', 'think', 'thought', 'time', 'times', 'use', 'used', 'using', 'want', 'way', 'work', 'works', 'worth', 'year', 'years']\n",
      "================================================================================\n",
      "\n",
      "Text cleaning functions defined\n",
      "Using optimized batch processing with nlp.pipe()\n",
      "Using lemmatization (spaCy) - NOT stemming (as per ZUM guidelines)\n",
      "Domain-specific stopwords DATA-DRIVEN from EDA frequency analysis\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"IDENTIFYING DOMAIN-SPECIFIC STOPWORDS (Data-Driven)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "negative_corpus = df[df['label'] == 0]['text'].tolist()\n",
    "positive_corpus = df[df['label'] == 1]['text'].tolist()\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    max_features=100,\n",
    "    stop_words='english',\n",
    "    token_pattern=r'\\b[a-zA-Z]{3,}\\b' \n",
    ")\n",
    "\n",
    "X_neg = vectorizer.fit_transform(negative_corpus)\n",
    "neg_words = set(vectorizer.get_feature_names_out())\n",
    "\n",
    "X_pos = vectorizer.fit_transform(positive_corpus)\n",
    "pos_words = set(vectorizer.get_feature_names_out())\n",
    "\n",
    "overlapping_words = neg_words.intersection(pos_words)\n",
    "\n",
    "print(f\"\\nTop 100 words in NEGATIVE reviews: {len(neg_words)}\")\n",
    "print(f\"Top 100 words in POSITIVE reviews: {len(pos_words)}\")\n",
    "print(f\"Overlapping words (domain-specific): {len(overlapping_words)}\")\n",
    "\n",
    "print(f\"\\nOverlapping words (appear in BOTH sentiment classes):\")\n",
    "print(f\"   {sorted(overlapping_words)}\")\n",
    "\n",
    "sentiment_indicators = {\n",
    "    'great', 'excellent', 'good', 'best', 'love', 'perfect', 'happy',\n",
    "    'bad', 'poor', 'worst', 'terrible', 'horrible', 'disappointing', 'awful'\n",
    "}\n",
    "\n",
    "domain_stopwords = overlapping_words - sentiment_indicators\n",
    "\n",
    "domain_stopwords.update({\n",
    "    'amazon', 'product', 'products', 'item', 'items',\n",
    "    'buy', 'bought', 'purchase', 'purchased', 'order', 'ordered',\n",
    "    'price', 'shipping', 'delivery'\n",
    "})\n",
    "\n",
    "all_stopwords = stop_words.union(domain_stopwords)\n",
    "\n",
    "print(f\"\\nFINAL STOPWORD CONFIGURATION:\")\n",
    "print(f\"   Standard English stopwords:  {len(stop_words)}\")\n",
    "print(f\"   Domain-specific stopwords:   {len(domain_stopwords)}\")\n",
    "print(f\"   Total combined stopwords:    {len(all_stopwords)}\")\n",
    "\n",
    "print(f\"\\nDomain-specific stopwords being removed:\")\n",
    "print(f\"   {sorted(domain_stopwords)}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def pre_clean(text):\n",
    "    \"\"\"\n",
    "    Fast regex-based pre-cleaning before SpaCy processing.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'<br\\s*/>', ' ', text)\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    text = re.sub(r'http\\S+|www\\S+|@\\S+|#\\S+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "print(\"\\nText cleaning functions defined\")\n",
    "print(\"Using optimized batch processing with nlp.pipe()\")\n",
    "print(\"Using lemmatization (spaCy)\")\n",
    "print(\"Domain-specific stopwords DATA-DRIVEN from EDA frequency analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f640faf",
   "metadata": {},
   "source": [
    "### Test Funkcji Czyszczącej\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f84eaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BEFORE vs AFTER Cleaning Example:\n",
      "================================================================================\n",
      "\n",
      "BEFORE (Raw Text):\n",
      "I LOVE this game! My husband and I hadn't played Final Fantasy games since the old Nintendo versions, but we bought this when we bought a PlayStation 2 for our son for Xmas. Wow! Have these games changed! They were fun before, but now they are visually amazing too and still incredibly fun! We have stressful lives, but for a few minutes (which can easily turn into a few hours) a day we can travel to distant lands, gather treasure, fight monsters successfully, spend money at all sorts of shops, and quest to save the world -- all without getting out of your PJs! Where else can you do all this over a period of several months for 50 bucks?? We'll definitely buy FF 11 when we are done with this one\n",
      "\n",
      "AFTER PRE-CLEAN (Regex):\n",
      "i love this game my husband and i hadnt played final fantasy games since the old nintendo versions but we bought this when we bought a playstation for our son for xmas wow have these games changed they were fun before but now they are visually amazing too and still incredibly fun we have stressful lives but for a few minutes which can easily turn into a few hours a day we can travel to distant lands gather treasure fight monsters successfully spend money at all sorts of shops and quest to save the world all without getting out of your pjs where else can you do all this over a period of several months for bucks well definitely buy ff when we are done with this one\n",
      "\n",
      "AFTER FULL CLEAN (Lemmatized + Domain Stopwords Removed):\n",
      "love husband final fantasy since nintendo version playstation son xmas wow change fun visually amazing still incredibly fun stressful life minute easily turn hour travel distant land gather treasure fight monster successfully spend money sort shop quest save world without get pjs else period several month buck well definitely one\n",
      "\n",
      "================================================================================\n",
      "Statistics:\n",
      "  Original length:  701 characters, 132 words\n",
      "  Cleaned length:   331 characters, 50 words\n",
      "  Reduction:        52.8%\n",
      "  Stopwords used:   270 (198 standard + 78 domain-specific)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "sample_text = df['text'].iloc[0]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"BEFORE vs AFTER Cleaning Example:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nBEFORE (Raw Text):\")\n",
    "print(f\"{sample_text}\\n\")\n",
    "\n",
    "pre_cleaned = pre_clean(sample_text)\n",
    "print(f\"AFTER PRE-CLEAN (Regex):\")\n",
    "print(f\"{pre_cleaned}\\n\")\n",
    "doc = nlp(pre_cleaned)\n",
    "lemmatized_tokens = [token.lemma_ for token in doc if token.lemma_ not in all_stopwords and len(token.lemma_) > 2]\n",
    "cleaned_sample = ' '.join(lemmatized_tokens)\n",
    "\n",
    "print(f\"AFTER FULL CLEAN (Lemmatized + Domain Stopwords Removed):\")\n",
    "print(f\"{cleaned_sample}\\n\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"Statistics:\")\n",
    "print(f\"  Original length:  {len(sample_text)} characters, {len(sample_text.split())} words\")\n",
    "print(f\"  Cleaned length:   {len(cleaned_sample)} characters, {len(cleaned_sample.split())} words\")\n",
    "print(f\"  Reduction:        {(1 - len(cleaned_sample)/len(sample_text))*100:.1f}%\")\n",
    "print(f\"  Stopwords used:   {len(all_stopwords)} ({len(stop_words)} standard + {len(domain_stopwords)} domain-specific)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798b1774",
   "metadata": {},
   "source": [
    "## 4. Zastosowanie Czyszczenia Tekstu\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c331d813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "OPTIMIZED TEXT CLEANING PIPELINE\n",
      "================================================================================\n",
      "\n",
      "Step 1: Applying regex pre-cleaning (fast)...\n",
      "Pre-cleaned 50,000 texts\n",
      "\n",
      "Step 2: Applying SpaCy lemmatization with nlp.pipe() (optimized batch processing)...\n",
      "(This will take approximately 3-5 minutes for 50,000 reviews)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95e0cc074bde4e5680523ae6ce86d2f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Text cleaning complete!\n",
      "================================================================================\n",
      "Domain-specific stopwords removed: 78 terms\n",
      "Total stopwords filtered: 270 terms\n",
      "\n",
      "BEFORE vs AFTER Cleaning Examples:\n",
      "\n",
      "--- Example 1 ---\n",
      "BEFORE: I LOVE this game! My husband and I hadn't played Final Fantasy games since the old Nintendo versions...\n",
      "AFTER:  love husband final fantasy since nintendo version playstation son xmas wow change fun visually amazi...\n",
      "\n",
      "--- Example 2 ---\n",
      "BEFORE: Just watch PBS This is not an exercise video. It is a collection of Elmo segments from Sesame Street...\n",
      "AFTER:  watch pbs exercise video collection elmo segment sesame street son get move two...\n",
      "\n",
      "--- Example 3 ---\n",
      "BEFORE: Black Spots on Vitamins I purchased Yummi Bears Multi-Vitamin & Mineral for my three year old daught...\n",
      "AFTER:  black spot vitamin yummi bear multivitamin mineral three daughter discover vitamin cover black spot ...\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"OPTIMIZED TEXT CLEANING PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nStep 1: Applying regex pre-cleaning (fast)...\")\n",
    "texts_to_process = df['text'].astype(str).apply(pre_clean).tolist()\n",
    "print(f\"Pre-cleaned {len(texts_to_process):,} texts\")\n",
    "print(\"\\nStep 2: Applying SpaCy lemmatization with nlp.pipe() (optimized batch processing)...\")\n",
    "print(\"(This will take approximately 3-5 minutes for 50,000 reviews)\\n\")\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "clean_texts = []\n",
    "for doc in tqdm(nlp.pipe(texts_to_process, batch_size=2000), total=len(texts_to_process), desc=\"Processing\"):\n",
    "    tokens = [token.lemma_ for token in doc if token.lemma_ not in all_stopwords and len(token.lemma_) > 2]\n",
    "    clean_texts.append(' '.join(tokens))\n",
    "\n",
    "df['cleaned_text'] = clean_texts\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Text cleaning complete!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Domain-specific stopwords removed: {len(domain_stopwords)} terms\")\n",
    "print(f\"Total stopwords filtered: {len(all_stopwords)} terms\")\n",
    "\n",
    "print(\"\\nBEFORE vs AFTER Cleaning Examples:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"BEFORE: {df['text'].iloc[i][:100]}...\")\n",
    "    print(f\"AFTER:  {df['cleaned_text'].iloc[i][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dbd538",
   "metadata": {},
   "source": [
    "## 5.  Podział Train/Validation/Test\n",
    "\n",
    "Wykonanie podziału **70% Trening / 15% Walidacja / 15% Test** ze stratyfikacją w celu utrzymania balansu klas.\n",
    "\n",
    "**Dlaczego Stratyfikowany?**\n",
    "- Zapewnia, że wszystkie trzy zbiory mają ten sam rozkład klas (50/50 pozytywne/negatywne)\n",
    "- Dla zbalansowanej ewaluacji na wszystkich etapach\n",
    "\n",
    "**Dlaczego Trzy Zbiory?**\n",
    "- **Trening (70%):** Dopasowanie parametrów modelu\n",
    "- **Walidacja (15%):** Dostrajanie hiperparametrów, early stopping \n",
    "- **Test (15%):**   ewaluacja "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e4600f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Train/Validation/Test Split Summary:\n",
      "============================================================\n",
      "Training set size:     35,000 samples (70.0%)\n",
      "Validation set size:   7,500 samples (15.0%)\n",
      "Test set size:         7,500 samples (15.0%)\n",
      "Total:                 50,000 samples\n",
      "\n",
      "Training label distribution:\n",
      "label\n",
      "0    17500\n",
      "1    17500\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation label distribution:\n",
      "label\n",
      "0    3750\n",
      "1    3750\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test label distribution:\n",
      "label\n",
      "0    3750\n",
      "1    3750\n",
      "Name: count, dtype: int64\n",
      "============================================================\n",
      "\n",
      "Stratification check (all should be ~0.50):\n",
      "  Train positive ratio:      0.5000\n",
      "  Validation positive ratio: 0.5000\n",
      "  Test positive ratio:       0.5000\n",
      "  Max difference: 0.000000\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "X = df['cleaned_text']\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.3,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.5,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(\"Train/Validation/Test Split Summary:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Training set size:     {len(X_train):,} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"Validation set size:   {len(X_val):,} samples ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set size:         {len(X_test):,} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "print(f\"Total:                 {len(X):,} samples\")\n",
    "\n",
    "print(f\"\\nTraining label distribution:\")\n",
    "print(y_train.value_counts().sort_index())\n",
    "\n",
    "print(f\"\\nValidation label distribution:\")\n",
    "print(y_val.value_counts().sort_index())\n",
    "\n",
    "print(f\"\\nTest label distribution:\")\n",
    "print(y_test.value_counts().sort_index())\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "train_pos_ratio = (y_train == 1).sum() / len(y_train)\n",
    "val_pos_ratio = (y_val == 1).sum() / len(y_val)\n",
    "test_pos_ratio = (y_test == 1).sum() / len(y_test)\n",
    "\n",
    "print(f\"\\nStratification check (all should be ~0.50):\")\n",
    "print(f\"  Train positive ratio:      {train_pos_ratio:.4f}\")\n",
    "print(f\"  Validation positive ratio: {val_pos_ratio:.4f}\")\n",
    "print(f\"  Test positive ratio:       {test_pos_ratio:.4f}\")\n",
    "print(f\"  Max difference: {max(abs(train_pos_ratio - val_pos_ratio), abs(train_pos_ratio - test_pos_ratio), abs(val_pos_ratio - test_pos_ratio)):.6f}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf727de",
   "metadata": {},
   "source": [
    "## 6. Wektoryzacja TF-IDF (dla Klasycznego ML)\n",
    "\n",
    "\n",
    "- To będzie wejście dla naszego modelu Regresji Logistycznej (Model A)\n",
    "- Sieci neuronowe i Transformery będą używać różnych reprezentacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c2c508c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TF-IDF vectorizer on TRAINING data ONLY (prevents data leakage)...\n",
      "\n",
      "============================================================\n",
      "TF-IDF Vectorization Summary:\n",
      "============================================================\n",
      "Vocabulary size: 5,000 unique terms\n",
      "\n",
      "Training matrix shape:   (35000, 5000)\n",
      "  - 35,000 samples\n",
      "  - 5,000 features\n",
      "\n",
      "Validation matrix shape: (7500, 5000)\n",
      "  - 7,500 samples\n",
      "  - 5,000 features\n",
      "\n",
      "Test matrix shape:       (7500, 5000)\n",
      "  - 7,500 samples\n",
      "  - 5,000 features\n",
      "\n",
      "Matrix sparsity: 99.52% (mostly zeros - efficient storage)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "print(\"Fitting TF-IDF vectorizer on TRAINING data ONLY (prevents data leakage)...\\n\")\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf.transform(X_val)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(\"TF-IDF Vectorization Summary:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Vocabulary size: {len(tfidf.vocabulary_):,} unique terms\")\n",
    "print(f\"\\nTraining matrix shape:   {X_train_tfidf.shape}\")\n",
    "print(f\"  - {X_train_tfidf.shape[0]:,} samples\")\n",
    "print(f\"  - {X_train_tfidf.shape[1]:,} features\")\n",
    "print(f\"\\nValidation matrix shape: {X_val_tfidf.shape}\")\n",
    "print(f\"  - {X_val_tfidf.shape[0]:,} samples\")\n",
    "print(f\"  - {X_val_tfidf.shape[1]:,} features\")\n",
    "print(f\"\\nTest matrix shape:       {X_test_tfidf.shape}\")\n",
    "print(f\"  - {X_test_tfidf.shape[0]:,} samples\")\n",
    "print(f\"  - {X_test_tfidf.shape[1]:,} features\")\n",
    "\n",
    "train_sparsity = (1 - X_train_tfidf.nnz / (X_train_tfidf.shape[0] * X_train_tfidf.shape[1])) * 100\n",
    "print(f\"\\nMatrix sparsity: {train_sparsity:.2f}% (mostly zeros - efficient storage)\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1e785f",
   "metadata": {},
   "source": [
    "### 6.5 Walidacja Wyników EDA w Słownictwie TF-IDF\n",
    "\n",
    "Weryfikacja, czy kluczowe bigramy zidentyfikowane w analizie N-gramów Notebooka 1 są przechwycone przez nasz wektoryzator TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73712aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "VALIDATING EDA N-GRAM FINDINGS IN TF-IDF VOCABULARY\n",
      "================================================================================\n",
      "\n",
      "NEGATIVE SENTIMENT BIGRAMS:\n",
      "--------------------------------------------------------------------------------\n",
      "  FOUND:   'waste money'\n",
      "  FOUND:   'customer service'\n",
      "  MISSING: 'poor quality' (may be filtered by min_df/max_df or stopword removal)\n",
      "  MISSING: 'not worth' (may be filtered by min_df/max_df or stopword removal)\n",
      "  MISSING: 'waste time' (may be filtered by min_df/max_df or stopword removal)\n",
      "  MISSING: 'return product' (may be filtered by min_df/max_df or stopword removal)\n",
      "  MISSING: 'don't waste' (may be filtered by min_df/max_df or stopword removal)\n",
      "  FOUND:   'completely useless'\n",
      "\n",
      "Negative bigrams found: 3/8 (38%)\n",
      "\n",
      "POSITIVE SENTIMENT BIGRAMS:\n",
      "--------------------------------------------------------------------------------\n",
      "  MISSING: 'highly recommend' (may be filtered by min_df/max_df or stopword removal)\n",
      "  MISSING: 'great product' (may be filtered by min_df/max_df or stopword removal)\n",
      "  MISSING: 'love it' (may be filtered by min_df/max_df or stopword removal)\n",
      "  MISSING: 'excellent quality' (may be filtered by min_df/max_df or stopword removal)\n",
      "  MISSING: 'well worth' (may be filtered by min_df/max_df or stopword removal)\n",
      "  MISSING: 'amazing product' (may be filtered by min_df/max_df or stopword removal)\n",
      "  MISSING: 'perfectly fine' (may be filtered by min_df/max_df or stopword removal)\n",
      "  FOUND:   'absolutely love'\n",
      "\n",
      "Positive bigrams found: 1/8 (12%)\n",
      "\n",
      "================================================================================\n",
      "ACTUAL TOP BIGRAMS IN TF-IDF VOCABULARY (Sample)\n",
      "================================================================================\n",
      "Total bigrams in vocabulary: 930\n",
      "\n",
      "Sample of 30 bigrams:\n",
      "   1. able get\n",
      "   2. able see\n",
      "   3. absolutely love\n",
      "   4. absolutely nothing\n",
      "   5. abuse child\n",
      "   6. actually get\n",
      "   7. add collection\n",
      "   8. ago still\n",
      "   9. agree review\n",
      "  10. agree reviewer\n",
      "  11. alarm clock\n",
      "  12. almost every\n",
      "  13. almost impossible\n",
      "  14. also come\n",
      "  15. also enjoy\n",
      "  16. also find\n",
      "  17. also get\n",
      "  18. also give\n",
      "  19. also good\n",
      "  20. also great\n",
      "  21. also include\n",
      "  22. also love\n",
      "  23. also one\n",
      "  24. always get\n",
      "  25. always good\n",
      "  26. always love\n",
      "  27. another brand\n",
      "  28. another great\n",
      "  29. another one\n",
      "  30. another reviewer\n",
      "\n",
      "================================================================================\n",
      "INSIGHT:\n",
      "================================================================================\n",
      "Some EDA bigrams may be missing due to:\n",
      "  1. Domain stopword removal ('product', 'service' removed)\n",
      "  2. min_df=2 filter (bigrams appearing in <2 documents)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"VALIDATING EDA N-GRAM FINDINGS IN TF-IDF VOCABULARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "vocabulary = set(tfidf.get_feature_names_out())\n",
    "\n",
    "expected_negative_bigrams = [\n",
    "    'waste money', 'customer service', 'poor quality', 'not worth',\n",
    "    'waste time', 'return product', 'don\\'t waste', 'completely useless'\n",
    "]\n",
    "\n",
    "expected_positive_bigrams = [\n",
    "    'highly recommend', 'great product', 'love it', 'excellent quality',\n",
    "    'well worth', 'amazing product', 'perfectly fine', 'absolutely love'\n",
    "]\n",
    "\n",
    "print(\"\\nNEGATIVE SENTIMENT BIGRAMS:\")\n",
    "print(\"-\" * 80)\n",
    "found_neg = 0\n",
    "for bigram in expected_negative_bigrams:\n",
    "    if bigram in vocabulary:\n",
    "        print(f\"  FOUND:   '{bigram}'\")\n",
    "        found_neg += 1\n",
    "    else:\n",
    "        print(f\"  MISSING: '{bigram}' (may be filtered by min_df/max_df or stopword removal)\")\n",
    "\n",
    "print(f\"\\nNegative bigrams found: {found_neg}/{len(expected_negative_bigrams)} ({found_neg/len(expected_negative_bigrams)*100:.0f}%)\")\n",
    "\n",
    "print(\"\\nPOSITIVE SENTIMENT BIGRAMS:\")\n",
    "print(\"-\" * 80)\n",
    "found_pos = 0\n",
    "for bigram in expected_positive_bigrams:\n",
    "    if bigram in vocabulary:\n",
    "        print(f\"  FOUND:   '{bigram}'\")\n",
    "        found_pos += 1\n",
    "    else:\n",
    "        print(f\"  MISSING: '{bigram}' (may be filtered by min_df/max_df or stopword removal)\")\n",
    "\n",
    "print(f\"\\nPositive bigrams found: {found_pos}/{len(expected_positive_bigrams)} ({found_pos/len(expected_positive_bigrams)*100:.0f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ACTUAL TOP BIGRAMS IN TF-IDF VOCABULARY (Sample)\")\n",
    "print(\"=\"*80)\n",
    "bigrams_in_vocab = [f for f in vocabulary if ' ' in f]\n",
    "print(f\"Total bigrams in vocabulary: {len(bigrams_in_vocab)}\")\n",
    "print(f\"\\nSample of 30 bigrams:\")\n",
    "for i, bg in enumerate(sorted(bigrams_in_vocab)[:30], 1):\n",
    "    print(f\"  {i:2d}. {bg}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INSIGHT:\")\n",
    "print(\"=\"*80)\n",
    "print(\"Some EDA bigrams may be missing due to:\")\n",
    "print(\"  1. Domain stopword removal ('product', 'service' removed)\")\n",
    "print(\"  2. min_df=2 filter (bigrams appearing in <2 documents)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b3251e",
   "metadata": {},
   "source": [
    "## 7. Zapis Przetworzonych Danych i Artefaktów\n",
    "\n",
    "Zapisanie wszystkich przetworzonych danych i modeli do użycia w Notebooku 3 (Trening).\n",
    "\n",
    "**Pliki wyjściowe:**\n",
    "- `train.csv`, `val.csv`, `test.csv`: Oczyszczone podziały tekstu (dla NN/Transformera)\n",
    "- `X_train_tfidf.pkl`, `X_val_tfidf.pkl`, `X_test_tfidf.pkl`: Cechy TF-IDF (dla Regresji Logistycznej)\n",
    "- `y_train.pkl`, `y_val.pkl`, `y_test.pkl`: Etykiety dla wszystkich podziałów\n",
    "- `tfidf_vectorizer.pkl`: Dopasowany wektoryzator TF-IDF (dla wnioskowania/wdrożenia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75ba802c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving processed data and artifacts...\n",
      "\n",
      "Saving TF-IDF matrices...\n",
      "Saving labels...\n",
      "Saving TF-IDF vectorizer...\n",
      "Saving EDA statistics (from Notebook 1)...\n",
      "Saving domain stopwords list...\n",
      "Saving cleaned text CSVs...\n",
      "\n",
      "================================================================================\n",
      "All files saved successfully!\n",
      "================================================================================\n",
      "Output directory: data/processed/\n",
      "\n",
      "CSV Files (for NN/Transformer):\n",
      "  - train.csv      (35,000 rows)\n",
      "  - val.csv        (7,500 rows)\n",
      "  - test.csv       (7,500 rows)\n",
      "\n",
      "TF-IDF Matrices (for Logistic Regression):\n",
      "  - X_train_tfidf.pkl ((35000, 5000))\n",
      "  - X_val_tfidf.pkl   ((7500, 5000))\n",
      "  - X_test_tfidf.pkl  ((7500, 5000))\n",
      "\n",
      "Labels:\n",
      "  - y_train.pkl (35,000 labels)\n",
      "  - y_val.pkl   (7,500 labels)\n",
      "  - y_test.pkl  (7,500 labels)\n",
      "\n",
      "Vectorizer:\n",
      "  - tfidf_vectorizer.pkl (vocabulary size: 5,000)\n",
      "\n",
      "EDA Statistics (from Notebook 1):\n",
      "  - eda_stats.pkl / eda_stats.json\n",
      "    • Recommended max_length: 160 words\n",
      "    • Mean word count: 77.6 words\n",
      "    • 95th percentile: 160 words\n",
      "\n",
      "Domain Stopwords:\n",
      "  - domain_stopwords.pkl (78 terms removed)\n",
      "\n",
      "================================================================================\n",
      "Preprocessing complete! Ready for Notebook 3 (Model Training)\n",
      "================================================================================\n",
      "\n",
      "Final Data Integrity Check:\n",
      "  Total samples: 50,000\n",
      "  Expected:      50,000\n",
      "  Match: YES\n",
      "\n",
      "  Train balance: {1: 17500, 0: 17500}\n",
      "  Val balance:   {0: 3750, 1: 3750}\n",
      "  Test balance:  {0: 3750, 1: 3750}\n",
      "\n",
      "================================================================================\n",
      "KEY INSIGHTS CARRIED FORWARD FROM EDA:\n",
      "================================================================================\n",
      "Domain stopwords: 78 neutral terms identified and removed\n",
      "Bigrams preserved: TF-IDF uses (1,2)-grams per EDA N-gram analysis\n",
      "Padding guidance: max_length=160 for 95% coverage\n"
     ]
    }
   ],
   "source": [
    "output_dir = 'data/processed'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"Saving processed data and artifacts...\\n\")\n",
    "\n",
    "print(\"Saving TF-IDF matrices...\")\n",
    "with open(f'{output_dir}/X_train_tfidf.pkl', 'wb') as f:\n",
    "    pickle.dump(X_train_tfidf, f)\n",
    "    \n",
    "with open(f'{output_dir}/X_val_tfidf.pkl', 'wb') as f:\n",
    "    pickle.dump(X_val_tfidf, f)\n",
    "    \n",
    "with open(f'{output_dir}/X_test_tfidf.pkl', 'wb') as f:\n",
    "    pickle.dump(X_test_tfidf, f)\n",
    "\n",
    "print(\"Saving labels...\")\n",
    "with open(f'{output_dir}/y_train.pkl', 'wb') as f:\n",
    "    pickle.dump(y_train, f)\n",
    "    \n",
    "with open(f'{output_dir}/y_val.pkl', 'wb') as f:\n",
    "    pickle.dump(y_val, f)\n",
    "    \n",
    "with open(f'{output_dir}/y_test.pkl', 'wb') as f:\n",
    "    pickle.dump(y_test, f)\n",
    "\n",
    "print(\"Saving TF-IDF vectorizer...\")\n",
    "with open(f'{output_dir}/tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf, f)\n",
    "\n",
    "print(\"Saving EDA statistics (from Notebook 1)...\")\n",
    "with open(f'{output_dir}/eda_stats.pkl', 'wb') as f:\n",
    "    pickle.dump(EDA_STATS, f)\n",
    "\n",
    "import json\n",
    "with open(f'{output_dir}/eda_stats.json', 'w') as f:\n",
    "    json.dump(EDA_STATS, f, indent=2)\n",
    "\n",
    "print(\"Saving domain stopwords list...\")\n",
    "with open(f'{output_dir}/domain_stopwords.pkl', 'wb') as f:\n",
    "    pickle.dump(domain_stopwords, f)\n",
    "\n",
    "print(\"Saving cleaned text CSVs...\")\n",
    "train_df = pd.DataFrame({\n",
    "    'text': X_train.values,\n",
    "    'label': y_train.values\n",
    "})\n",
    "\n",
    "val_df = pd.DataFrame({\n",
    "    'text': X_val.values,\n",
    "    'label': y_val.values\n",
    "})\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    'text': X_test.values,\n",
    "    'label': y_test.values\n",
    "})\n",
    "\n",
    "train_df.to_csv(f'{output_dir}/train.csv', index=False)\n",
    "val_df.to_csv(f'{output_dir}/val.csv', index=False)\n",
    "test_df.to_csv(f'{output_dir}/test.csv', index=False)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"All files saved successfully!\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Output directory: {output_dir}/\\n\")\n",
    "\n",
    "print(f\"CSV Files (for NN/Transformer):\")\n",
    "print(f\"  - train.csv      ({len(train_df):,} rows)\")\n",
    "print(f\"  - val.csv        ({len(val_df):,} rows)\")\n",
    "print(f\"  - test.csv       ({len(test_df):,} rows)\")\n",
    "\n",
    "print(f\"\\nTF-IDF Matrices (for Logistic Regression):\")\n",
    "print(f\"  - X_train_tfidf.pkl ({X_train_tfidf.shape})\")\n",
    "print(f\"  - X_val_tfidf.pkl   ({X_val_tfidf.shape})\")\n",
    "print(f\"  - X_test_tfidf.pkl  ({X_test_tfidf.shape})\")\n",
    "\n",
    "print(f\"\\nLabels:\")\n",
    "print(f\"  - y_train.pkl ({len(y_train):,} labels)\")\n",
    "print(f\"  - y_val.pkl   ({len(y_val):,} labels)\")\n",
    "print(f\"  - y_test.pkl  ({len(y_test):,} labels)\")\n",
    "\n",
    "print(f\"\\nVectorizer:\")\n",
    "print(f\"  - tfidf_vectorizer.pkl (vocabulary size: {len(tfidf.vocabulary_):,})\")\n",
    "\n",
    "print(f\"\\nEDA Statistics (from Notebook 1):\")\n",
    "print(f\"  - eda_stats.pkl / eda_stats.json\")\n",
    "print(f\"    • Recommended max_length: {EDA_STATS['recommended_max_length']} words\")\n",
    "print(f\"    • Mean word count: {EDA_STATS['mean_word_count']:.1f} words\")\n",
    "print(f\"    • 95th percentile: {EDA_STATS['percentile_95']} words\")\n",
    "\n",
    "print(f\"\\nDomain Stopwords:\")\n",
    "print(f\"  - domain_stopwords.pkl ({len(domain_stopwords)} terms removed)\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Preprocessing complete! Ready for Notebook 3 (Model Training)\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\nFinal Data Integrity Check:\")\n",
    "print(f\"  Total samples: {len(train_df) + len(val_df) + len(test_df):,}\")\n",
    "print(f\"  Expected:      {len(df):,}\")\n",
    "print(f\"  Match: {'YES' if len(train_df) + len(val_df) + len(test_df) == len(df) else 'NO'}\")\n",
    "print(f\"\\n  Train balance: {train_df['label'].value_counts().to_dict()}\")\n",
    "print(f\"  Val balance:   {val_df['label'].value_counts().to_dict()}\")\n",
    "print(f\"  Test balance:  {test_df['label'].value_counts().to_dict()}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"KEY INSIGHTS CARRIED FORWARD FROM EDA:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Domain stopwords: {len(domain_stopwords)} neutral terms identified and removed\")\n",
    "print(f\"Bigrams preserved: TF-IDF uses (1,2)-grams per EDA N-gram analysis\")\n",
    "print(f\"Padding guidance: max_length={EDA_STATS['recommended_max_length']} for 95% coverage\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
